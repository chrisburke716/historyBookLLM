"""Mock LLM provider for testing purposes."""

import asyncio
from collections.abc import AsyncIterator
from typing import Any

from ...data_models.entities import ChatMessage, MessageRole
from ..config import LLMConfig
from ..interfaces.llm_interface import LLMInterface
from ..utils import (
    estimate_token_count,
)


class MockLLMProvider(LLMInterface):
    """Mock LLM provider for testing and development."""

    def __init__(self, config: LLMConfig | None = None):
        """
        Initialize the mock provider.

        Args:
            config: LLM configuration (not used in mock but kept for interface compatibility)
        """
        self.config = config or LLMConfig()
        self.response_delay = 0.1  # Simulate response time

    async def generate_response(
        self, messages: list[ChatMessage], context: str | None = None, **kwargs: Any
    ) -> str:
        """Generate a mock response."""
        # Simulate processing time
        await asyncio.sleep(self.response_delay)

        if not messages:
            return "I don't have any messages to respond to."

        last_message = messages[-1]

        # Create a mock response based on the last message
        if context:
            response = (
                f"Based on the historical context provided, I can tell you that "
                f"regarding your question '{last_message.content}', the historical "
                f"documents suggest several important points. This is a mock response "
                f"that would normally be generated by an actual LLM."
            )
        else:
            response = (
                f"Thank you for your question: '{last_message.content}'. "
                f"This is a mock response from the development LLM provider. "
                f"In production, this would be replaced with an actual LLM response."
            )

        return response

    async def generate_stream_response(
        self, messages: list[ChatMessage], context: str | None = None, **kwargs: Any
    ) -> AsyncIterator[str]:
        """Generate a mock streaming response."""
        response = await self.generate_response(messages, context, **kwargs)

        # Split response into chunks and yield them
        words = response.split()
        chunk_size = 3  # Words per chunk

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i : i + chunk_size])
            if i + chunk_size < len(words):
                chunk += " "

            await asyncio.sleep(0.05)  # Simulate streaming delay
            yield chunk

    def count_tokens(self, text: str) -> int:
        """Count tokens using estimation."""
        return estimate_token_count(text)

    def validate_messages(self, messages: list[ChatMessage]) -> bool:
        """Validate message format."""
        if not messages:
            return False

        for msg in messages:
            if not isinstance(msg, ChatMessage):
                return False
            if not msg.content or not msg.content.strip():
                return False
            if msg.role not in [
                MessageRole.USER,
                MessageRole.ASSISTANT,
                MessageRole.SYSTEM,
            ]:
                return False

        return True
